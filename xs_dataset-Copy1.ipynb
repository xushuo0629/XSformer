{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T14:06:47.393201Z",
     "start_time": "2022-10-18T14:06:47.373993Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import scipy.io as scio\n",
    "import math\n",
    "import time\n",
    "from models.XSformer  import XSformer\n",
    "from util.data_loader import My_dataset\n",
    "from util.epoch_timer import epoch_time\n",
    "\"\"\"\n",
    "参数表\n",
    "\"\"\"\n",
    "filename = 'dataset_12000_re.mat'\n",
    "dataNum = 12000\n",
    "# model para\n",
    "nhead           = 2\n",
    "d_model         = 6\n",
    "dim_feedforward = 64\n",
    "dropout         = 0.1\n",
    "n_layers        = 3\n",
    "mlp_hidden      = 16\n",
    "LR              = 0.1\n",
    "batchsize       = 3200\n",
    "# adam para\n",
    "init_lr         = 1e-5\n",
    "weight_decay    = 5e-4\n",
    "adam_eps        = 5e-9\n",
    "# scheduler para\n",
    "factor          = 0.9\n",
    "patience        = 10\n",
    "# others\n",
    "warmup          = 100\n",
    "epoch           = 10\n",
    "clip            = 1.0\n",
    "inf             = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T14:06:48.746309Z",
     "start_time": "2022-10-18T14:06:48.722309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,873 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-a9618ef25542>:9: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XSformer(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=6, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=6, bias=True)\n",
       "          (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=6, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=6, bias=True)\n",
       "          (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=6, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=6, bias=True)\n",
       "          (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=6, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=6, bias=True)\n",
       "          (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=6, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=6, bias=True)\n",
       "          (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=6, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=6, bias=True)\n",
       "          (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (mlp1): Linear(in_features=6, out_features=16, bias=True)\n",
       "  (mlp2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (ac): LeakyReLU(negative_slope=0.1)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "设置模型，优化器，损失函数\n",
    "\"\"\"\n",
    "model    = XSformer(nhead           = nhead,\n",
    "                    d_model         = d_model,\n",
    "                    dim_feedforward = dim_feedforward,\n",
    "                    dropout         = dropout,\n",
    "                    n_layers        = n_layers,\n",
    "                    mlp_hidden      = mlp_hidden,\n",
    "                    LR              = LR)\n",
    "optimizer = Adam(params       = model.parameters(),\n",
    "                 lr           = init_lr,\n",
    "                 weight_decay = weight_decay,\n",
    "                 eps          = adam_eps)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)\n",
    "criterion = nn.L1Loss()\n",
    "# 输出模型大小并初始化        \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model.apply(initialize_weights)  \n",
    "#out = model(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T14:06:50.241096Z",
     "start_time": "2022-10-18T14:06:50.222024Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "subfunction\n",
    "\"\"\"\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "        \n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg, out) in enumerate(iterator):\n",
    "        #src = batch.src\n",
    "        #trg = batch.trg\n",
    "        # reshape data\n",
    "        src = src.view(-1,4,d_model)  #  batch*24       ---->   batch* len(4)* d_model(6)\n",
    "        trg = trg.unsqueeze(1)  #  batch*d_model        ---->   batch* len(1)* d_model(6)\n",
    "        x = torch.tensor(src, dtype=torch.float)\n",
    "        y = torch.tensor(trg, dtype=torch.float)\n",
    "        z = torch.tensor(out, dtype=torch.float)   \n",
    "        # \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #output = model(src, trg)\n",
    "        output = model(x, y)\n",
    "        #output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "        #trg = trg[:, 1:].contiguous().view(-1)\n",
    "        out = out.contiguous().view(-1,out.size(-1))  # batch*1*1 ----> batch*1\n",
    "        z = z.contiguous().view(-1,1)               # batch ----> batch*1\n",
    "        \n",
    "        loss = criterion(output, z)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def run(total_epoch, best_loss):\n",
    "    train_losses, test_losses, bleus = [], [], []\n",
    "    for step in range(total_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss = train(model, train_batch, optimizer, criterion, clip)\n",
    "        end_time = time.time()\n",
    "        #if step > warmup:\n",
    "        #   scheduler.step(valid_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        #test_losses.append(valid_loss)\n",
    "        #bleus.append(bleu)\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        f = open('result/train_loss.txt', 'w')\n",
    "        f.write(str(train_losses))\n",
    "        f.close()        \n",
    "        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T14:06:51.563256Z",
     "start_time": "2022-10-18T14:06:51.385004Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing done\n"
     ]
    }
   ],
   "source": [
    "loader = My_dataset(filename,d_model,dataNum)\n",
    "trainSet, validSet, testSet = loader.make_dataset(dataNum)\n",
    "train_batch, valid_batch, test_batch = loader.make_iter(trainSet, validSet, testSet,\n",
    "                                                     batchsize=batchsize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T14:06:57.875791Z",
     "start_time": "2022-10-18T14:06:52.517627Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-a9618ef25542>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(src, dtype=torch.float)\n",
      "<ipython-input-21-a9618ef25542>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(trg, dtype=torch.float)\n",
      "<ipython-input-21-a9618ef25542>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(out, dtype=torch.float)\n",
      "C:\\Users\\xushuo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:97: UserWarning: Using a target size (torch.Size([3200])) that is different to the input size (torch.Size([3200, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0.0 % , loss : 12.950315475463867\n",
      "step : 33.33 % , loss : 12.859456062316895\n",
      "step : 66.67 % , loss : 12.883637428283691\n",
      "Epoch: 1 | Time: 0m 0s\n",
      "\tTrain Loss: 12.898 | Train PPL: 399433.666\n",
      "step : 0.0 % , loss : 12.944225311279297\n",
      "step : 33.33 % , loss : 12.849969863891602\n",
      "step : 66.67 % , loss : 12.851941108703613\n",
      "Epoch: 2 | Time: 0m 0s\n",
      "\tTrain Loss: 12.882 | Train PPL: 393188.896\n",
      "step : 0.0 % , loss : 12.926239013671875\n",
      "step : 33.33 % , loss : 12.843324661254883\n",
      "step : 66.67 % , loss : 12.878366470336914\n",
      "Epoch: 3 | Time: 0m 0s\n",
      "\tTrain Loss: 12.883 | Train PPL: 393424.076\n",
      "step : 0.0 % , loss : 12.909133911132812\n",
      "step : 33.33 % , loss : 12.827638626098633\n",
      "step : 66.67 % , loss : 12.842483520507812\n",
      "Epoch: 4 | Time: 0m 0s\n",
      "\tTrain Loss: 12.860 | Train PPL: 384520.360\n",
      "step : 0.0 % , loss : 12.90361213684082\n",
      "step : 33.33 % , loss : 12.82951831817627\n",
      "step : 66.67 % , loss : 12.812938690185547\n",
      "Epoch: 5 | Time: 0m 0s\n",
      "\tTrain Loss: 12.849 | Train PPL: 380290.120\n",
      "step : 0.0 % , loss : 12.897095680236816\n",
      "step : 33.33 % , loss : 12.84277629852295\n",
      "step : 66.67 % , loss : 12.814318656921387\n",
      "Epoch: 6 | Time: 0m 0s\n",
      "\tTrain Loss: 12.851 | Train PPL: 381321.023\n",
      "step : 0.0 % , loss : 12.876991271972656\n",
      "step : 33.33 % , loss : 12.780502319335938\n",
      "step : 66.67 % , loss : 12.800925254821777\n",
      "Epoch: 7 | Time: 0m 0s\n",
      "\tTrain Loss: 12.819 | Train PPL: 369340.015\n",
      "step : 0.0 % , loss : 12.845783233642578\n",
      "step : 33.33 % , loss : 12.792552947998047\n",
      "step : 66.67 % , loss : 12.756277084350586\n",
      "Epoch: 8 | Time: 0m 0s\n",
      "\tTrain Loss: 12.798 | Train PPL: 361567.643\n",
      "step : 0.0 % , loss : 12.8555269241333\n",
      "step : 33.33 % , loss : 12.792695999145508\n",
      "step : 66.67 % , loss : 12.780973434448242\n",
      "Epoch: 9 | Time: 0m 0s\n",
      "\tTrain Loss: 12.810 | Train PPL: 365759.802\n",
      "step : 0.0 % , loss : 12.830487251281738\n",
      "step : 33.33 % , loss : 12.76073169708252\n",
      "step : 66.67 % , loss : 12.762162208557129\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 12.784 | Train PPL: 356632.239\n"
     ]
    }
   ],
   "source": [
    "run(total_epoch=epoch, best_loss=inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
